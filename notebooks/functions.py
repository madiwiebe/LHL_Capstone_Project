import pandas as pd
import os
import math

def temp():
    print('hello')


# define path variable for folder containing folders of all the raw .csv data
path = r"D:\School\LHL\capstone_project_data\raw_datasets\pcds_data" 

# function to read all files within a folder
def read_files(folder_name):
    # create list which will contain all dataframes generated by .csv files within the folder
    data_frames = []
    # create list of file names contained within the folder
    file_names = os.listdir(f"{path}/{folder_name}")
    # iterate through files within each folder
    for file_name in file_names:
        # ignore 'variable' .csv file in each folder
        if file_name == 'variables.csv':
            continue
        # print function status
        print(f'Reading {file_name}')
        # read each .csv to a dataframe
        station_data_df = pd.read_csv(f"{path}/{folder_name}/{file_name}", skiprows=[0], na_values=[' None','None'])
        # add filename as a new column
        station_data_df['filename'] = file_name
        # append dataframe to list
        data_frames.append(station_data_df)
    return data_frames


# function to concatenate all of a single folder's dataframes
def concat_folder_dfs(folder_df_list):
    all_folder_dfs = pd.concat(folder_df_list, join='outer', ignore_index=True)
    return all_folder_dfs


# function to calculate the air density of humid air
def calculate_air_density(Pressure_hPa, Temp_C, RelativeHumidity):
    SpecificGasConstantDryAir = 287.0531
    SpecificGasConstantWaterVapour = 461.4964

    Pressure_Pa = Pressure_hPa * 100
    Temp_K = Temp_C + 273.15

    # Computing Es_hPa
    Eso = 6.1078
    c0 = 0.99999683
    c1 = -0.90826951*10**-2
    c2 = 0.78736169*10**-4
    c3 = -0.61117958*10**-6
    c4 = 0.43884187*10**-8
    c5 = -0.29883885*10**-10
    c6 = 0.21874425*10**-12
    c7 = -0.17892321*10**-14
    c8 = 0.11112018*10**-16
    c9 = -0.30994571*10**-19

    p = c0 + Temp_C * (c1 + Temp_C * (c2 + Temp_C * (c3 + Temp_C *
                        (c4 + Temp_C * (c5 + Temp_C * (c6 + Temp_C * (c7 + Temp_C * (c8 + Temp_C * c9))))))))

    Es_hPa = Eso / (p**8)
    Es_Pa = Es_hPa * 100

    PartialPressureWaterVapour_Pa = Es_Pa * RelativeHumidity
    PartialPressureDryAir_Pa = Pressure_Pa - PartialPressureWaterVapour_Pa

    DensityHumidAir_Pa = (PartialPressureDryAir_Pa / (SpecificGasConstantDryAir * Temp_K) + 
                        PartialPressureWaterVapour_Pa / (SpecificGasConstantWaterVapour * Temp_K))
    
    return DensityHumidAir_Pa


# function to calculate wind energy from air density and wind speed
def calculate_wind_energy(air_density, wind_speed):
    avg_wind_efficiency = 0.3
    avg_blade_length_m = 2.15
    pi = math.pi

    wind_energy_W = 0.5 * avg_wind_efficiency * air_density * pi * (avg_blade_length_m**2) * (wind_speed**3)

    return wind_energy_W


# function to calculate solar energy from solar radiation
def calculate_solar_energy(solar_radiation):
    avg_solar_panel_area_m2 = 1.7
    avg_solar_panel_efficiency = 0.163
    avg_performance_ratio = 0.8592
    # solar_radiation must be in units of W/m**2

    solar_energy_W = avg_solar_panel_area_m2 * avg_solar_panel_efficiency * solar_radiation * avg_performance_ratio

    return solar_energy_W

# function to read .csv into dataframe
def to_df(file_name):
    # remove .csv from variable name
    data_name = file_name.split('.csv')[0]
    # define path variable for data folder containing concatenated .csv files
    path_2 = '../data'
    # read file
    df_name = pd.read_csv(f'{path_2}/{data_name}.csv', index_col=0)

    return df_name

# function to identify column names
def list_columns(df_name):
    # list column names
    column_names = list(df_name.columns)
    
    return column_names

# function to count null values
def count_nulls(df_name):    
    # count null values
    num_nulls = df_name.isna().sum()

    return num_nulls

# function to check for duplicates
def count_row_duplicates(df_name):
    duplicate_rows = df_name.duplicated()
    duplicate_index = duplicate_rows[duplicate_rows].index
    
    return duplicate_index

# function to perform all three
def start_cleaning(df_name):
    df_columns = list_columns(df_name)
    df_nulls = count_nulls(df_name)
    df_duplicates = count_row_duplicates(df_name)

    return df_columns, df_nulls, df_duplicates


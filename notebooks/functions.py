import pandas as pd
import os
import math
import psycopg2

def temp():
    print('hello')


# define path variable for folder containing folders of all the raw .csv data
path = r"D:\School\LHL\capstone_project_data\raw_datasets\pcds_data" 

# define another path variable for processed data
path_2 = '../data'

# define an explicit path variable for processed data (required for pgAdmin4 to locate system files)
path_3 = r"D:\School\LHL\capstone_project_data\LHL_Capstone_Project\data"

# define a path variable for folder containing cleaned .csv data
path_clean = r"D:\School\LHL\capstone_project_data\raw_datasets\pcds_data_clean"

# function to clean .csv files
def clean_csv(folder_name):
    # define path variable for folders that will contain all the cleaned .csv data
    path_clean = r"D:\School\LHL\capstone_project_data\raw_datasets\pcds_data_clean"+f"\{folder_name}"
    # check whether folder to contain cleaned csv exists or not
    isExist = os.path.exists(f'{path_clean}')
    # create directory if it does not already exist
    if not isExist:
        os.makedirs(f'{path_clean}')
    # create list of file names contained within the raw folder
    file_names = os.listdir(f"{path}/{folder_name}")
    # iterate through files within each folder
    for file_name in file_names:
        # ignore 'variable' .csv file in each folder
        if file_name == 'variables.csv':
            continue
        # print function status
        print(f'Reading {file_name}')
        # read each .csv to a dataframe
        station_data_df = pd.read_csv(f"{path}/{folder_name}/{file_name}", skiprows=[0], na_values=[' None','None'])
        # remove leading and trailing white space from column names
        station_data_df.rename(columns=lambda x: x.strip(), inplace=True)
        # make all columns lowercase
        station_data_df.rename(columns=lambda y: y.lower(), inplace=True)
        # replace other white space with an underscore
        station_data_df.rename(columns=lambda z: z.replace(' ','_'), inplace=True)
        # add filename as a new column
        station_data_df['station_id'] = file_name.split('.csv')[0]
        # add network id as a new column
        station_data_df['network_id'] = folder_name
        # save station data back to csv in new folder
        station_data_df.to_csv(f"{path_clean}/{file_name}", index=False)


# function to read all files within a folder
def read_files_to_df_list(folder_name):
    # create list which will contain all dataframes generated by .csv files within the folder
    data_frames = []
    # create list of file names contained within the folder
    file_names = os.listdir(f"{path_clean}/{folder_name}")
    # iterate through files within each folder
    for file_name in file_names:
        # ignore 'variable' .csv file in each folder
        if file_name == 'variables.csv':
            continue
        # print function status
        print(f'Reading {file_name}')
        # read each .csv to a dataframe
        station_data_df = pd.read_csv(f"{path_clean}/{folder_name}/{file_name}")
        # # add filename as a new column
        # station_data_df['filename'] = file_name
        # append dataframe to list
        data_frames.append(station_data_df)
    return data_frames


# function to concatenate all of a single folder's dataframes
def concat_folder_dfs(folder_df_list):
    all_folder_dfs = pd.concat(folder_df_list, join='outer', ignore_index=True)
    return all_folder_dfs


# # function to calculate the air density of humid air
# def calculate_air_density(Pressure_hPa, Temp_C, RelativeHumidity):
#     SpecificGasConstantDryAir = 287.0531
#     SpecificGasConstantWaterVapour = 461.4964

#     Pressure_Pa = Pressure_hPa * 100
#     Temp_K = Temp_C + 273.15

#     # Computing Es_hPa
#     Eso = 6.1078
#     c0 = 0.99999683
#     c1 = -0.90826951*10**-2
#     c2 = 0.78736169*10**-4
#     c3 = -0.61117958*10**-6
#     c4 = 0.43884187*10**-8
#     c5 = -0.29883885*10**-10
#     c6 = 0.21874425*10**-12
#     c7 = -0.17892321*10**-14
#     c8 = 0.11112018*10**-16
#     c9 = -0.30994571*10**-19

#     p = c0 + Temp_C * (c1 + Temp_C * (c2 + Temp_C * (c3 + Temp_C *
#                         (c4 + Temp_C * (c5 + Temp_C * (c6 + Temp_C * (c7 + Temp_C * (c8 + Temp_C * c9))))))))

#     Es_hPa = Eso / (p**8)
#     Es_Pa = Es_hPa * 100

#     PartialPressureWaterVapour_Pa = Es_Pa * RelativeHumidity
#     PartialPressureDryAir_Pa = Pressure_Pa - PartialPressureWaterVapour_Pa

#     DensityHumidAir_Pa = (PartialPressureDryAir_Pa / (SpecificGasConstantDryAir * Temp_K) + 
#                         PartialPressureWaterVapour_Pa / (SpecificGasConstantWaterVapour * Temp_K))
    
#     return DensityHumidAir_Pa


# # function to calculate wind energy from air density and wind speed
# def calculate_wind_energy(air_density, wind_speed):
#     avg_wind_efficiency = 0.3
#     avg_blade_length_m = 2.15
#     pi = math.pi

#     wind_energy_W = 0.5 * avg_wind_efficiency * air_density * pi * (avg_blade_length_m**2) * (wind_speed**3)

#     return wind_energy_W

# function to calculate hours of daylight per day by month
def hours_of_sunlight(df):
    if pd.DatetimeIndex(df['time']).month == 1:
        hos = 7.5
    elif pd.DatetimeIndex(df['time']).month == 2:
        hos = 9
    elif pd.DatetimeIndex(df['time']).month == 3:
        hos = 11
    elif pd.DatetimeIndex(df['time']).month == 4:
        hos = 13
    elif pd.DatetimeIndex(df['time']).month == 5:
        hos = 15
    elif pd.DatetimeIndex(df['time']).month == 6:
        hos = 16.75
    elif pd.DatetimeIndex(df['time']).month == 7:
        hos = 17
    elif pd.DatetimeIndex(df['time']).month == 8:
        hos = 15.75
    elif pd.DatetimeIndex(df['time']).month == 9:
        hos = 13.75
    elif pd.DatetimeIndex(df['time']).month == 10:
        hos = 11.5
    elif pd.DatetimeIndex(df['time']).month == 11:
        hos = 9.5
    else: hos = 7.75
    return hos

# function to calculate solar energy from solar radiation
def calculate_solar_energy(df):
    avg_solar_panel_area_m2 = 1.7
    avg_solar_panel_efficiency = 0.163
    avg_performance_ratio = 0.8592
    num_panels = 20
    # solar_radiation must be in units of W/m**2

    solar_energy_W = avg_solar_panel_area_m2 * avg_solar_panel_efficiency * df['solar_radiation'] * avg_performance_ratio * num_panels

    return solar_energy_W

# function to calculate average daily solar energy production (kWh)
def avg_daily_solar_kWh(df):
    
    hos = hours_of_sunlight(df)
    solar_energy_W = calculate_solar_energy(df)
    for day in df['time']:
        print(f'Calculating daily solar energy average for {day}')
        df['daily_avg_kWh'] = (df['solar_energy_W'].groupby(['network_id','station_id']).sum() / df.COUNT(solar_energy_W)) / 1000 * hos
        

# function to read .csv into dataframe
def to_df(file_name):
    # remove .csv from variable name
    data_name = file_name.split('.csv')[0]
    # define path variable for data folder containing concatenated .csv files
    path_2 = '../data'
    # read file
    df_name = pd.read_csv(f'{path_2}/{data_name}.csv')

    return df_name

# function to identify column names
def list_columns(df_name):
    # list column names
    column_names = list(df_name.columns)
    
    return column_names

# function to count null values
def count_nulls(df_name):    
    # count null values
    num_nulls = df_name.isna().sum()

    return num_nulls

# function to check for duplicates
def count_row_duplicates(df_name):
    duplicate_rows = df_name.duplicated()
    duplicate_index = duplicate_rows[duplicate_rows].index
    
    return duplicate_index

# function to perform all three
def start_cleaning(df_name):
    df_columns = list_columns(df_name)
    df_nulls = count_nulls(df_name)
    df_duplicates = count_row_duplicates(df_name)

    return df_columns, df_nulls, df_duplicates

# function that contains database connection info
def connect_to_db():
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()   


# function to create an empty table in Postgres database
def create_sql_table(file_name):
    table_name = file_name.split('.csv')[0]
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()
    cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
    sql = f"CREATE TABLE {table_name}();"
    print(f"Creating {table_name} in database")
    cursor.execute(sql)
    con.commit()


# function to determine dtypes of csv columns
def find_dtypes(files):
    dtype_dict = {}
    for file in files:
        col_names = pd.read_csv(f'{path_2}/{file}', nrows=1)
        file_dtype_dict = dict(col_names.dtypes)
        dtype_dict.update(file_dtype_dict)
    return dtype_dict

# function to add columns to tables in Postgres database
def add_sql_columms(file_name, path):
    table_name = file_name.split('.csv')[0]
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()
    column_names = pd.read_csv(f'{path}/{file_name}', nrows=1)
    col_name_list = list(column_names)
    col_name_no_spaces = [sub.replace(' ','_') for sub in col_name_list]
    dtype_list = list(column_names.dtypes)
    for i in range(len(dtype_list)):
        if dtype_list[i] == 'object':
            dtype_list[i] = 'varchar'
        if dtype_list[i] == 'float64':
            dtype_list[i] = 'float'
        if dtype_list[i] == 'int64':
            dtype_list[i] = 'varchar'

    for i in range(len(col_name_no_spaces)):
        # if col_name_no_spaces[i] == 'time':
        #     dtype_list[i] = 'timestamp'
        sql = f"""
            ALTER TABLE {table_name}
            ADD COLUMN IF NOT EXISTS {col_name_no_spaces[i]} {dtype_list[i]};
            """
        print(f"Adding columns to {table_name}")
        cursor.execute(sql)
        con.commit()


# function to add values to columns in Postgres database
def add_values(file_name, path):
    table_name = file_name.split('.csv')[0]
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()
    column_names = pd.read_csv(f'{path}/{file_name}', nrows=0)
    col_name_list = list(column_names)
    col_name_no_spaces = [sub.replace(' ','_') for sub in col_name_list]
    column_name_string = ','.join(col_name_no_spaces)
    sql = f"""
        COPY {table_name}({column_name_string})
        FROM '{path}/{file_name}'
        DELIMITER ','
        CSV HEADER;
    """
    print(f"Adding values to {table_name}")
    cursor.execute(sql)
    con.commit()

# function to create a set of column names
def create_col_name_set(folder):
    # create empty set of column names
    col_name_set = set()
    # create list of file names contained within the folder
    file_names = os.listdir(f"{path_clean}/{folder}")
    # iterate through files within each folder
    for file_name in file_names:
        #print(f"Processing folder: {folder}, file: {file_name}")
        column_names = pd.read_csv(f"{path_clean}/{folder}/{file_name}", nrows=1)
        file_col_name_list = list(column_names.columns)
        col_name_set.update(set(file_col_name_list))
    return col_name_set

# function to concatenate all files within a network folder
def combine_data(network_name):
    print(f'reading {network_name} files')
    df_list = read_files_to_df_list(network_name)
    print(f'concatenating {network_name} dataframes')
    df_combined = concat_folder_dfs(df_list)
    print(f'saving {network_name} data to csv')
    df_combined.to_csv(f'../data/{network_name}_data_clean.csv', index=False)

# function to copy all data from a list of tables into a destination table
def merge_tables(table_list, destination_table):
    
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()

    for table in table_list:
        table_name = table.split('.csv')[0]
        sql1 = f"""ALTER TABLE {table_name}
                ALTER COLUMN time
                TYPE timestamp
                USING time::timestamp without time zone;
        """
        print(f"Converting 'time' column in {table_name} to timestamp")
        cursor.execute(sql1)
        con.commit()

        column_names = pd.read_csv(f'{path_2}/{table}', nrows=1)
        col_name_list = list(column_names)
        column_name_list_string = ','.join(col_name_list) 

        sql2 = f"""
            INSERT INTO {destination_table}({column_name_list_string})
            SELECT {column_name_list_string}
            FROM {table_name};
        """
        print(f"Adding values from {table_name} to {destination_table}")

        cursor.execute(sql2)
        con.commit()
        print(f"Successfully added values to {destination_table}")


# function to join metadata
def create_joined_table(table_name):

    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()

    sql = f"""
    DROP TABLE IF EXISTS {table_name}_final; 
    
    CREATE TABLE {table_name}_final AS
    SELECT *
    FROM {table_name}
    JOIN station_metadata_clean
    USING(network_id, station_id);
    """
    print(f"Successfully joined metadata with {table_name}")

    cursor.execute(sql)
    con.commit()

# function to close connection to pgAdmin database
def close_conn():
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.close()    

# function to calculate min, max, avg of a column
def max_min_avg(table_name, column_name):
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()

    sql = f"""
    SELECT MAX({column_name}), MIN({column_name}), AVG({column_name})
    FROM {table_name};
    """
    print(f"Fetching results for {column_name}...")

    cursor.execute(sql)
    result = cursor.fetchone()
    return result

# function to count null values in a column
def count_na(table_name, column_name):
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()

    sql = f"""
    SELECT COUNT(*) 
    FROM {table_name}  
    WHERE {column_name} IS NULL;
    """
    print(f"Counting null values for {column_name}...")

    cursor.execute(sql)
    result = cursor.fetchone()
    result1 = result[0]
    return result1   

# function to fill null values in a column with its average
def fill_na_with_avg(table_name, column_name):
    postgres_password = os.environ['POSTGRES_PASS']
    con = psycopg2.connect(
        database='lhl_capstone_project',
        user='postgres',
        password=f'{postgres_password}',
        host='localhost',
        port='5432'
    )
    con.rollback()
    cursor = con.cursor()

    sql1 = f"""
    SELECT AVG({column_name})
    FROM {table_name}
    """
    print(f"Calculating average for {column_name}...")
    cursor.execute(sql1)
    result = cursor.fetchone()
    result1 = result[0]

    sql2 = f"""
    UPDATE {table_name} 
    SET {column_name} = {result1} 
    WHERE {column_name} IS NULL;
    """
    print(f"Filling null values for {column_name}...")

    cursor.execute(sql2)
    con.commit()
    